{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'legs': {0: {'fins': {0.0: {'toothed': {0.0: 7.0, 1.0: 3.0}},\n",
      "                       1.0: {'eggs': {0.0: 1.0, 1.0: 4.0}}}},\n",
      "          2: {'hair': {0.0: 2.0, 1.0: 1.0}},\n",
      "          4: {'hair': {0.0: {'toothed': {0.0: 7.0, 1.0: 5.0}}, 1.0: 1.0}},\n",
      "          6: {'aquatic': {0.0: 6.0, 1.0: 7.0}},\n",
      "          8: 7.0}}\n",
      "The prediction accuracy is:  85.71428571428571 %\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from pprint import pprint  \n",
    "#Import the dataset and define the feature as well as the target datasets / columns#  \n",
    "dataset = pd.read_csv('C:/Users/hp/Python_programming/zoo.csv',  \n",
    "                      names=['animal_name','hair','feathers','eggs','milk',  \n",
    "                                                   'airbone','aquatic','predator','toothed','backbone',  \n",
    "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])#Import all columns omitting the fist which consists the names of the animals  \n",
    "#We drop the animal names since this is not a good feature to split the data on  \n",
    "dataset=dataset.drop('animal_name',axis=1)  \n",
    "  \n",
    "def entropy(target_col):  \n",
    "    \n",
    "    elements,counts = np.unique(target_col,return_counts = True)  \n",
    "    entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])  \n",
    "    return entropy  \n",
    "  \n",
    "def InfoGain(data,split_attribute_name,target_name=\"class\"):  \n",
    "         \n",
    "    #Calculate the entropy of the total dataset  \n",
    "    total_entropy = entropy(data[target_name])  \n",
    "      \n",
    "    ##Calculate the entropy of the dataset  \n",
    "      \n",
    "    #Calculate the values and the corresponding counts for the split attribute   \n",
    "    vals,counts= np.unique(data[split_attribute_name],return_counts=True)  \n",
    "      \n",
    "    #Calculate the weighted entropy  \n",
    "    Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dropna()[target_name]) for i in range(len(vals))])  \n",
    "      \n",
    "    #Calculate the information gain  \n",
    "    Information_Gain = total_entropy - Weighted_Entropy  \n",
    "    return Information_Gain  \n",
    "  \n",
    "def ID3(data,originaldata,features,target_attribute_name=\"class\",parent_node_class = None):  \n",
    "  \n",
    "    #Define the stopping criteria --> If one of this is satisfied, we want to return a leaf node#  \n",
    "      \n",
    "    #If all target_values have the same value, return this value  \n",
    "    if len(np.unique(data[target_attribute_name])) <= 1:  \n",
    "        return np.unique(data[target_attribute_name])[0]  \n",
    "      \n",
    "    #If the dataset is empty, return the mode target feature value in the original dataset  \n",
    "    elif len(data)==0:  \n",
    "        return np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribute_name],return_counts=True)[1])]  \n",
    "      \n",
    "    #If the feature space is empty, return the mode target feature value of the direct parent node --> Note that  \n",
    "    #the direct parent node is that node which has called the current run of the ID3 algorithm and hence  \n",
    "    #the mode target feature value is stored in the parent_node_class variable.  \n",
    "      \n",
    "    elif len(features) ==0:  \n",
    "        return parent_node_class  \n",
    "      \n",
    "    #If none of the above holds true, grow the tree!  \n",
    "      \n",
    "    else:  \n",
    "        #Set the default value for this node --> The mode target feature value of the current node  \n",
    "        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],return_counts=True)[1])]  \n",
    "          \n",
    "        #Select the feature which best splits the dataset  \n",
    "        item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset  \n",
    "        best_feature_index = np.argmax(item_values)  \n",
    "        best_feature = features[best_feature_index]  \n",
    "          \n",
    "        #Create the tree structure. The root gets the name of the feature (best_feature) with the maximum information  \n",
    "        #gain in the first run  \n",
    "        tree = {best_feature:{}}  \n",
    "          \n",
    "          \n",
    "        #Remove the feature with the best inforamtion gain from the feature space  \n",
    "        features = [i for i in features if i != best_feature]  \n",
    "          \n",
    "        #Grow a branch under the root node for each possible value of the root node feature  \n",
    "          \n",
    "        for value in np.unique(data[best_feature]):  \n",
    "            value = value  \n",
    "            #Split the dataset along the value of the feature with the largest information gain and therwith create sub_datasets  \n",
    "            sub_data = data.where(data[best_feature] == value).dropna()  \n",
    "              \n",
    "            #Call the ID3 algorithm for each of those sub_datasets with the new parameters --> Here the recursion comes in!  \n",
    "            subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)  \n",
    "              \n",
    "            #Add the sub tree, grown from the sub_dataset to the tree under the root node  \n",
    "            tree[best_feature][value] = subtree  \n",
    "              \n",
    "        return(tree)      \n",
    "                  \n",
    "def predict(query,tree,default = 1):  \n",
    "    \n",
    "    for key in list(query.keys()):  \n",
    "        if key in list(tree.keys()):  \n",
    "            \n",
    "            try:  \n",
    "                result = tree[key][query[key]]   \n",
    "            except:  \n",
    "                return default  \n",
    "    \n",
    "            result = tree[key][query[key]]  \n",
    "            \n",
    "            if isinstance(result,dict):  \n",
    "                return predict(query,result)  \n",
    "            else:  \n",
    "                return result  \n",
    "  \n",
    "def train_test_split(dataset):  \n",
    "    training_data = dataset.iloc[:80].reset_index(drop=True)#We drop the index respectively relabel the index  \n",
    "    #starting form 0, because we do not want to run into errors regarding the row labels / indexes  \n",
    "    testing_data = dataset.iloc[80:].reset_index(drop=True)  \n",
    "    return training_data,testing_data  \n",
    "  \n",
    "training_data = train_test_split(dataset)[0]  \n",
    "testing_data = train_test_split(dataset)[1]   \n",
    "  \n",
    "def test(data,tree):  \n",
    "    #Create new query instances by simply removing the target feature column from the original dataset and   \n",
    "    #convert it to a dictionary  \n",
    "    queries = data.iloc[:,:-1].to_dict(orient = \"records\")  \n",
    "      \n",
    "    #Create a empty DataFrame in whose columns the prediction of the tree are stored  \n",
    "    predicted = pd.DataFrame(columns=[\"predicted\"])   \n",
    "      \n",
    "    #Calculate the prediction accuracy  \n",
    "    for i in range(len(data)):  \n",
    "        predicted.loc[i,\"predicted\"] = predict(queries[i],tree,1.0)   \n",
    "    print('The prediction accuracy is: ',(np.sum(predicted[\"predicted\"] == data[\"class\"])/len(data))*100,'%')  \n",
    "      \n",
    "tree = ID3(training_data,training_data,training_data.columns[:-1])  \n",
    "pprint(tree)  \n",
    "test(testing_data,tree)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction accuracy is:  80.95238095238095 %\n"
     ]
    }
   ],
   "source": [
    "#Import the DecisionTreeClassifier  \n",
    "from sklearn.tree import DecisionTreeClassifier  \n",
    "  \n",
    "#Import the dataset   \n",
    "dataset = pd.read_csv('C:/Users/hp/Python_programming/zoo.csv',    \n",
    "                      names=['animal_name','hair','feathers','eggs','milk',    \n",
    "                                                   'airbone','aquatic','predator','toothed','backbone',    \n",
    "                                                  'breathes','venomous','fins','legs','tail','domestic','catsize','class',])  \n",
    "#We drop the animal names since this is not a good feature to split the data on  \n",
    "dataset=dataset.drop('animal_name',axis=1)  \n",
    "  \n",
    "train_features = dataset.iloc[:80,:-1]  \n",
    "test_features = dataset.iloc[80:,:-1]  \n",
    "train_targets = dataset.iloc[:80,-1]  \n",
    "test_targets = dataset.iloc[80:,-1]  \n",
    "  \n",
    "tree = DecisionTreeClassifier(criterion = 'entropy').fit(train_features,train_targets)  \n",
    "  \n",
    "prediction = tree.predict(test_features)  \n",
    "  \n",
    "print(\"The prediction accuracy is: \",tree.score(test_features,test_targets)*100,\"%\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line args are ['C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-66fe54ec-bea4-4c27-94e1-c3347abb3ed9.json']: \n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-66e5ac0545b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-1-66e5ac0545b8>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Command line args are {}: \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_csv_to_header_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data_file'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-66e5ac0545b8>\u001b[0m in \u001b[0;36mload_config\u001b[1;34m(config_file)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 186\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    187\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mliteral_eval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "\n",
    "\n",
    "def load_csv_to_header_data(filename):\n",
    "    fpath = os.path.join(os.getcwd(), filename)\n",
    "    fs = csv.reader(open(fpath, newline='\\n'))\n",
    "\n",
    "    all_row = []\n",
    "    for r in fs:\n",
    "        all_row.append(r)\n",
    "\n",
    "    headers = all_row[0]\n",
    "    idx_to_name, name_to_idx = get_header_name_to_idx_maps(headers)\n",
    "\n",
    "    data = {\n",
    "        'header': headers,\n",
    "        'rows': all_row[1:],\n",
    "        'name_to_idx': name_to_idx,\n",
    "        'idx_to_name': idx_to_name\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_header_name_to_idx_maps(headers):\n",
    "    name_to_idx = {}\n",
    "    idx_to_name = {}\n",
    "    for i in range(0, len(headers)):\n",
    "        name_to_idx[headers[i]] = i\n",
    "        idx_to_name[i] = headers[i]\n",
    "    return idx_to_name, name_to_idx\n",
    "\n",
    "\n",
    "def project_columns(data, columns_to_project):\n",
    "    data_h = list(data['header'])\n",
    "    data_r = list(data['rows'])\n",
    "\n",
    "    all_cols = list(range(0, len(data_h)))\n",
    "\n",
    "    columns_to_project_ix = [data['name_to_idx'][name] for name in columns_to_project]\n",
    "    columns_to_remove = [cidx for cidx in all_cols if cidx not in columns_to_project_ix]\n",
    "\n",
    "    for delc in sorted(columns_to_remove, reverse=True):\n",
    "        del data_h[delc]\n",
    "        for r in data_r:\n",
    "            del r[delc]\n",
    "\n",
    "    idx_to_name, name_to_idx = get_header_name_to_idx_maps(data_h)\n",
    "\n",
    "    return {'header': data_h, 'rows': data_r,\n",
    "            'name_to_idx': name_to_idx,\n",
    "            'idx_to_name': idx_to_name}\n",
    "\n",
    "\n",
    "def get_uniq_values(data):\n",
    "    idx_to_name = data['idx_to_name']\n",
    "    idxs = idx_to_name.keys()\n",
    "\n",
    "    val_map = {}\n",
    "    for idx in iter(idxs):\n",
    "        val_map[idx_to_name[idx]] = set()\n",
    "\n",
    "    for data_row in data['rows']:\n",
    "        for idx in idx_to_name.keys():\n",
    "            att_name = idx_to_name[idx]\n",
    "            val = data_row[idx]\n",
    "            if val not in val_map.keys():\n",
    "                val_map[att_name].add(val)\n",
    "    return val_map\n",
    "\n",
    "\n",
    "def get_class_labels(data, target_attribute):\n",
    "    rows = data['rows']\n",
    "    col_idx = data['name_to_idx'][target_attribute]\n",
    "    labels = {}\n",
    "    for r in rows:\n",
    "        val = r[col_idx]\n",
    "        if val in labels:\n",
    "            labels[val] = labels[val] + 1\n",
    "        else:\n",
    "            labels[val] = 1\n",
    "    return labels\n",
    "\n",
    "\n",
    "def entropy(n, labels):\n",
    "    ent = 0\n",
    "    for label in labels.keys():\n",
    "        p_x = labels[label] / n\n",
    "        ent += - p_x * math.log(p_x, 2)\n",
    "    return ent\n",
    "\n",
    "\n",
    "def partition_data(data, group_att):\n",
    "    partitions = {}\n",
    "    data_rows = data['rows']\n",
    "    partition_att_idx = data['name_to_idx'][group_att]\n",
    "    for row in data_rows:\n",
    "        row_val = row[partition_att_idx]\n",
    "        if row_val not in partitions.keys():\n",
    "            partitions[row_val] = {\n",
    "                'name_to_idx': data['name_to_idx'],\n",
    "                'idx_to_name': data['idx_to_name'],\n",
    "                'rows': list()\n",
    "            }\n",
    "        partitions[row_val]['rows'].append(row)\n",
    "    return partitions\n",
    "\n",
    "\n",
    "def avg_entropy_w_partitions(data, splitting_att, target_attribute):\n",
    "    # find uniq values of splitting att\n",
    "    data_rows = data['rows']\n",
    "    n = len(data_rows)\n",
    "    partitions = partition_data(data, splitting_att)\n",
    "\n",
    "    avg_ent = 0\n",
    "\n",
    "    for partition_key in partitions.keys():\n",
    "        partitioned_data = partitions[partition_key]\n",
    "        partition_n = len(partitioned_data['rows'])\n",
    "        partition_labels = get_class_labels(partitioned_data, target_attribute)\n",
    "        partition_entropy = entropy(partition_n, partition_labels)\n",
    "        avg_ent += partition_n / n * partition_entropy\n",
    "\n",
    "    return avg_ent, partitions\n",
    "\n",
    "\n",
    "def most_common_label(labels):\n",
    "    mcl = max(labels, key=lambda k: labels[k])\n",
    "    return mcl\n",
    "\n",
    "\n",
    "def id3(data, uniqs, remaining_atts, target_attribute):\n",
    "    labels = get_class_labels(data, target_attribute)\n",
    "\n",
    "    node = {}\n",
    "\n",
    "    if len(labels.keys()) == 1:\n",
    "        node['label'] = next(iter(labels.keys()))\n",
    "        return node\n",
    "\n",
    "    if len(remaining_atts) == 0:\n",
    "        node['label'] = most_common_label(labels)\n",
    "        return node\n",
    "\n",
    "    n = len(data['rows'])\n",
    "    ent = entropy(n, labels)\n",
    "\n",
    "    max_info_gain = None\n",
    "    max_info_gain_att = None\n",
    "    max_info_gain_partitions = None\n",
    "\n",
    "    for remaining_att in remaining_atts:\n",
    "        avg_ent, partitions = avg_entropy_w_partitions(data, remaining_att, target_attribute)\n",
    "        info_gain = ent - avg_ent\n",
    "        if max_info_gain is None or info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            max_info_gain_att = remaining_att\n",
    "            max_info_gain_partitions = partitions\n",
    "\n",
    "    if max_info_gain is None:\n",
    "        node['label'] = most_common_label(labels)\n",
    "        return node\n",
    "\n",
    "    node['attribute'] = max_info_gain_att\n",
    "    node['nodes'] = {}\n",
    "\n",
    "    remaining_atts_for_subtrees = set(remaining_atts)\n",
    "    remaining_atts_for_subtrees.discard(max_info_gain_att)\n",
    "\n",
    "    uniq_att_values = uniqs[max_info_gain_att]\n",
    "\n",
    "    for att_value in uniq_att_values:\n",
    "        if att_value not in max_info_gain_partitions.keys():\n",
    "            node['nodes'][att_value] = {'label': most_common_label(labels)}\n",
    "            continue\n",
    "        partition = max_info_gain_partitions[att_value]\n",
    "        node['nodes'][att_value] = id3(partition, uniqs, remaining_atts_for_subtrees, target_attribute)\n",
    "\n",
    "    return node\n",
    "\n",
    "\n",
    "def load_config(config_file):\n",
    "    with open(config_file, 'r') as myfile:\n",
    "        data = myfile.read().replace('\\n', '')\n",
    "    return ast.literal_eval(data)\n",
    "\n",
    "\n",
    "def pretty_print_tree(root):\n",
    "    stack = []\n",
    "    rules = set()\n",
    "\n",
    "    def traverse(node, stack, rules):\n",
    "        if 'label' in node:\n",
    "            stack.append(' THEN ' + node['label'])\n",
    "            rules.add(''.join(stack))\n",
    "            stack.pop()\n",
    "        elif 'attribute' in node:\n",
    "            ifnd = 'IF ' if not stack else ' AND '\n",
    "            stack.append(ifnd + node['attribute'] + ' EQUALS ')\n",
    "            for subnode_key in node['nodes']:\n",
    "                stack.append(subnode_key)\n",
    "                traverse(node['nodes'][subnode_key], stack, rules)\n",
    "                stack.pop()\n",
    "            stack.pop()\n",
    "\n",
    "    traverse(root, stack, rules)\n",
    "    print(os.linesep.join(rules))\n",
    "\n",
    "\n",
    "def main():\n",
    "    argv = sys.argv\n",
    "    print(\"Command line args are {}: \".format(argv))\n",
    "\n",
    "    config = load_config(argv[1])\n",
    "\n",
    "    data = load_csv_to_header_data(config['data_file'])\n",
    "    data = project_columns(data, config['data_project_columns'])\n",
    "\n",
    "    target_attribute = config['target_attribute']\n",
    "    remaining_attributes = set(data['header'])\n",
    "    remaining_attributes.remove(target_attribute)\n",
    "\n",
    "    uniqs = get_uniq_values(data)\n",
    "\n",
    "    root = id3(data, uniqs, remaining_attributes, target_attribute)\n",
    "\n",
    "    pretty_print_tree(root)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
